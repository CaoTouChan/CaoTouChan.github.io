<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>结构化学习（3）：结构化 SVM</title>
  <meta name="description" content="上一篇文章 《结构化学习（2）：结构化线性模型》 提出一个线性模型来解决结构化学习。">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="结构化学习（3）：结构化 SVM">
  <meta name="twitter:description" content="上一篇文章 《结构化学习（2）：结构化线性模型》 提出一个线性模型来解决结构化学习。">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="结构化学习（3）：结构化 SVM">
  <meta property="og:description" content="上一篇文章 《结构化学习（2）：结构化线性模型》 提出一个线性模型来解决结构化学习。">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2019/05/struc-03/">
  <link rel="alternate" type="application/rss+xml" title="陈草头" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>

  <!-- 数学公式 -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      inlineMath: [['$','$']]
    }
  });
</script>


  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 陈草头 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="陈草头 logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for 陈草头" class="blog-button">陈草头</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">越名教而任自然</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">90 后 | 广东人 | 现居住于帝都 | 算法攻城狮</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        <!-- 
        <p class="panel-cover__description"><a href="https://caotouchan.github.io" target="_blank">我的主页</a></p>
         -->
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">博客</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  
  <!-- Weibo -->
  <li class="navigation__item">
    <a href="http://weibo.com/lookka520" title="@lookka520 的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li>
  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/caotouchan" title="@caotouchan 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:caotouchan@outlook.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-red"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2019-05-26 12:50:01 +0800" itemprop="datePublished" class="post-meta__date date">2019-05-26</time> &#8226; <span class="post-meta__tags tags"></span>
    </div>
    <h1 class="post-title">结构化学习（3）：结构化 SVM</h1>
  </header>

  <section class="post">
    <p>上一篇文章 <a href="https://caotouchan.github.io/2019/05/struc-02/">《结构化学习（2）：结构化线性模型》</a> 提出一个线性模型来解决结构化学习。</p>

<p>对于<strong>问题一</strong>（定义 $F(x,y)$），在这个模型中，我们预设 $F(x, y)$ 是 linear 的，其中加入了很多人工的特征。但是 <strong>如果 $F(x,y)$不是线性的呢 ？ 如果我们不想要做这么多人工特征工程呢 ？</strong></p>

<p>对于<strong>问题二</strong> (解决 arg max 问题)，我们<strong>穷举</strong>所有可能得 $y$。我们是否能找到一些方法简化这个问题？ 事实上，在不同问题上，我们可能会有不同的方法来应对这个问题。比如 ——</p>

<ul>
  <li>
    <p>Object Detection : Branch and Bound algorithm 、Selective Search</p>
  </li>
  <li>
    <p>Sequence Labeling : Viterbi Algorithm</p>
  </li>
</ul>

<p>使用什么算法来应对这个问题，取决于 $\phi(x,y)$。如果不知道如何解这个 QP，也可以使用 <strong>Genetic Algorithm</strong> 来尝试解决。</p>

<blockquote>
  <p>OPEN QUESTION: 如果 Inference 阶段无法得到一个确切的结果呢？</p>
</blockquote>

<p>对于 <strong>问题 3</strong> ，我们的目标是给定数据找到 $F(x,y)$）。下文将讲解这个问题。</p>

<h2 id="线性可分-structured-perceptron">线性可分 （Structured Perceptron）</h2>

<h3 id="算法描述">算法描述</h3>

<p><a href="http://www.caotouchan.tech/2019/05/25/%E7%BB%93%E6%9E%84%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%EF%BC%9A%E7%BB%93%E6%9E%84%E5%8C%96%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">上一篇文章</a> 中，我们对提出的线性模型的解决方案描述如下：</p>

<p>对于输入：
<script type="math/tex">\text { Input: training data set }\left\{\left(x^{1}, \hat{y}^{1}\right)\left(x^{2}, \hat{y}^{2}\right) \ldots,\left(x^{r}, \hat{y}^{r}\right) \ldots\right\}</script></p>

<p>我们的输出是：
<script type="math/tex">\text { Output: weight vector w }</script></p>

<p>算法如下：</p>

<blockquote>
  <p>$ 初始化 \space w = 0$</p>

  <p>$do : $</p>

  <p>$\space \space 对于每个训练样本对\space \left(x^{r}, \hat{y}^{r}\right) $</p>

  <p>$\space \space \space \space 找到标签\space \tilde{y}^{r} \space 以最大化 \space w \cdot \phi\left(x^{r}, y\right) ：$</p>

  <p>$\space \space \space \space\space \space \space \space \widetilde{y}^{r}=\arg \max _{y \in Y} w \cdot \phi\left(x^{r}, y\right)(\text { 问题 2 } )$</p>

  <p>$ \space \space \space \space \text { 如果 } \tilde{y}^{r} \neq \hat{y}^{r}, \text { 更新 } \mathbf{w} ： $</p>

  <p>$\space \space \space \space\space \space \space \space w \rightarrow w+\phi\left(x^{r}, \hat{y}^{r}\right)-\phi\left(x^{r}, \widetilde{y}^{r}\right) $</p>

  <p>$ 直到 w 不再更新 \rightarrow 结束循环$</p>
</blockquote>

<h3 id="收敛性及其证明">收敛性及其证明</h3>

<p>这个算法是否会收敛？实际上，它是否收敛和 $y$ 的空间没有关系：</p>

<blockquote>
  <p>在线性可分的案例中，只需要更新最多 $(R / \delta)^{2}$ 次，就能找到 $\widehat{W}$ 。
其中，$\delta$ 是间隔（ margin ），$R$ 是 $\phi(x, y)$ 和 $\phi\left(x, y^{\prime}\right)$ 之间的最大距离</p>
</blockquote>

<p>证明如下。</p>

<p>算法的更新公式是：</p>

<script type="math/tex; mode=display">w^{k}=w^{k-1}+\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, \widetilde{y}^{n}\right)</script>

<p>设$\delta$ 是间隔（ margin ），$R$ 是 $\phi(x, y)$ 和 $\phi\left(x, y^{\prime}\right)$ 之间的最大距离，$\rho_{\mathrm{k}} $ 是 $\hat{\boldsymbol{w}} $ 和 $\mathbf{w}^{\mathbf{k}}$ 之间的夹角，则：</p>

<script type="math/tex; mode=display">\cos \rho_{k}=\frac{\hat{w}}{\|\hat{w}\|} \cdot \frac{w^{k}}{\left\|w^{k}\right\|}</script>

<p>其中：</p>

<script type="math/tex; mode=display">\hat{w} \cdot w^{k}=\hat{w} \cdot\left(w^{k-1}+\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, \widetilde{y}^{n}\right)\right)</script>

<p>即：
<script type="math/tex">\hat{w} \cdot w^{k}= \hat{w} \cdot w^{k-1}+\hat{w} \cdot \phi\left(x^{n}, \hat{y}^{n}\right)-\hat{w} \cdot \phi\left(x^{n}, \tilde{y}^{n}\right)</script></p>

<p>所以：</p>

<script type="math/tex; mode=display">\hat{w} \cdot w^{k} \geq \hat{w} \cdot w^{k-1}+\delta</script>

<p>也就是：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\hat{w} \cdot w^{1} \geq \hat{w} \cdot w^{0}+\delta} \\ {\hat{w} \cdot w^{1} \geq \delta}\end{array}

\begin{array}{l}{\hat{w} \cdot w^{2} \geq \hat{w} \cdot w^{1}+\delta} \\ {\hat{w} \cdot w^{2} \geq 2 \delta}\end{array}

\begin{array}{l}{...} \\ {...}\end{array}</script>

<p>所以可以得到：</p>

<script type="math/tex; mode=display">\hat{w} \cdot w^{k} \geq k \delta</script>

<p>也就是 $\hat{w}$ 和 $w^{k}$ 的内积在增加。</p>

<p>不是一般性的，有：$|\widehat{W}|=1$ （ $\hat{w} \cdot \phi\left(x^{n}, \hat{y}^{n}\right) \geq \hat{w} \cdot \phi\left(x^{n}, y\right)+\delta$ 进行正则）</p>

<p>如果我们能<strong>证明 ${\left|w^{k}\right|}$ 增加不那么快</strong>，那么相当于知道 $\hat{w}$ 和 $w^{k}$  两者的距离逐渐接近 —— 越来越收敛。</p>

<p>由 ：</p>

<script type="math/tex; mode=display">w^{k}=w^{k-1}+\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, \widetilde{y}^{n}\right)</script>

<p>有：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\left\|w^{k}\right\|^{2}=\| w^{k-1}+\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left.\left(x^{n}, \widetilde{y}^{n}\right)\right|^{2}} \\ {=\left\|w^{k-1}\right\|^{2}+\| \phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left.\left(x^{n}, \widetilde{y}^{n}\right)\right|^{2}+2 w^{k-1} \cdot\left(\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, \widetilde{y}^{n}\right)\right)}\end{array}</script>

<p>因为更新过程是做 $arg \space max$ 操作，然后纠正方向，故</p>

<script type="math/tex; mode=display">w^{k-1} \cdot\left(\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, \widetilde{y}^{n}\right)\right) \lt 0</script>

<p>而</p>

<script type="math/tex; mode=display">\| \phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left.\left(x^{n}, \widetilde{y}^{n}\right)\right|^{2} \gt 0</script>

<p>由上可知：</p>

<script type="math/tex; mode=display">\left\|w^{k}\right\|^{2} \leq\left\|\boldsymbol{w}^{k-1}\right\|^{2}+\mathbf{R}^{2}</script>

<p>所以可以得到 $\left|w^{k}\right|^{2} $ 的 upper bound：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\left\|w^{1}\right\|^{2} \leq\left\|w^{0}\right\|^{2}+\mathrm{R}^{2}=\mathrm{R}^{2}} \\
{\left\|w^{2}\right\|^{2} \leq\left\|w^{1}\right\|^{2}+\mathrm{R}^{2} \leq 2 \mathrm{R}^{2}} \\
... \\
{\left\|w^{k}\right\|^{2} \leq k \mathrm{R}^{2}}\end{array}</script>

<p>到此，我们有：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \cos \rho_{k}=& \frac{\hat{w}}{\|\hat{w}\|} \cdot \frac{w^{k}}{\left\|w^{k}\right\|} \\ & \geq \frac{k \delta}{\sqrt{k R^{2}}}=\sqrt{k} \frac{\delta}{R} \end{aligned} %]]></script>

<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4gsyl1p5ej30qy0m0q4k.jpg" alt="" /></p>

<p>所以：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\sqrt{k} \frac{\delta}{R} \leq 1} \\ {k \leq\left(\frac{R}{\delta}\right)^{2}}\end{array}</script>

<p>到此，收敛性得以证明。</p>

<h2 id="非线性可分">非线性可分</h2>

<p>当数据非线性可分的时候，有些 weight vector 也会比其他的 weight vectors 好，如下图，$W^{\prime}$ 好于 $W^{\prime \prime}$：</p>

<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4gszdjr6tj30tw0go0tk.jpg" alt="" /></p>

<h3 id="定义代价函数">定义代价函数</h3>

<p>定义代价函数 $C$ 来衡量 $w$ 有多差，然后选择 $w$ 来最小化 $C$ 。</p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt0k4iyij31000giwfk.jpg" alt="" /></p>

<blockquote>
  <p>PS : 选择最大间距那个（$\max _{y}\left[w \cdot \phi\left(x^{n}, y\right)\right]$） 而不是前三个之和、前十个和的均值或者其他，是因为我们（假设）问题二的 $arg \space max$ 已经被解决。如果使用其他来进行计算，实际上是复杂化了这个问题。</p>
</blockquote>

<h3 id="随机梯度下降求解">随机梯度下降求解</h3>

<p>那么如何求解梯度 $\nabla C^{n}$ 呢 ？</p>

<p>考虑公式 $C^{n}=\max _{y}\left[w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)$ 中 $\max _{y}\left[w \cdot \phi\left(x^{n}, y\right)\right]$ ，$y$ 会随着 $w$ 的不同而变化。</p>

<p>假设下图是 $w$ 的空间，$\operatorname{argmax}_{y}\left[w \cdot \phi\left(x^{n}, y\right)\right]$ 在不同的$w$ 下对应不同的 $y$ ($y^{\prime}、y^{\prime\prime}、y^{\prime\prime\prime} … $)：</p>

<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4gt0y8e61j30uw0j6wev.jpg" alt="" /></p>

<p>也就是在各自的 region 中，$\operatorname{argmax}_{y}\left[w \cdot \phi\left(x^{n}, y\right)\right] = y^{\prime} 、y^{\prime\prime}、y^{\prime\prime\prime} … $。</p>

<p>我们知道，$y^{\prime} = w \cdot \phi\left(x^{n}, y^{\prime}\right) -w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)$，故 $\nabla C^{n} =\phi\left(x^{n}, y^{\prime}\right) -\phi\left(x^{n}, \hat{y}^{n}\right)$。$y^{\prime\prime}、y^{\prime\prime\prime} …$ 类似。也就是，在避开边界的地方，实际都是可微的。</p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt19k5ivj30y60jmdhh.jpg" alt="" /></p>

<p>所以，更新算法为：</p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt1jc0hzj30y60jw407.jpg" alt="" /></p>

<h2 id="衡量错误-修改代价函数">衡量错误 （修改代价函数）</h2>

<p>上文中我们把所有的错误视为一样的代价。但实际上不同的错误，可能有的错的少，有的错的多。所以，实际上把错误进行量化会更好。</p>

<h3 id="加入-margin">加入 margin</h3>

<p>这种衡量的标准在不同的 case 会有所差异。考虑下边的 case：</p>

<p>定义 $\Delta(\hat{y}, y) : \hat{y} \text { 和 } y \text { 之间的不同 } (&gt;0) $</p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt1ub0obj30z40g8wg1.jpg" alt="" /></p>

<p>修改下代价函数为：</p>

<script type="math/tex; mode=display">C^{n}=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \widehat{y}^{n}\right)</script>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt24pf29j30qw0hiwfb.jpg" alt="" /></p>

<p>在之前的算法中，我们在每一次迭代里选择训练数据 $ {x^{n}, \hat{y}^{n}} $：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\tilde{y}^{n}=\max _{y}\left[w \cdot \phi\left(x^{n}, y\right)\right]} \\ \\
{\nabla C^{n}(w)=\phi\left(x^{n}, \tilde{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)} \\ \\
{w \rightarrow w-\eta\left[\phi\left(x^{n}, \tilde{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)\right]}\end{array}</script>

<p>一句上述思想修改下：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\overline{y}^{n}=\arg \max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]} \\ \\
{\nabla C^{n}(w)=\phi\left(x^{n}, \overline{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)} \\ \\
{w \rightarrow w-\eta\left[\phi\left(x^{n}, \overline{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)\right]}\end{array}</script>

<blockquote>
  <p>上述方法实际上引入了一个新的问题 ： $\Delta\left(\hat{y}^{n}, y\right)$ 。所以设计 $\Delta$ 函数的时候需要考虑是否会使得问题更加复杂。</p>
</blockquote>

<h3 id="另一个视角">另一个视角</h3>

<p>最小化代价函数实际上（有可能）等于最小化训练集错误的 upper bound：</p>

<script type="math/tex; mode=display">C^{\prime}=\sum_{n=1}^{N} \Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right) \leq C=\sum_{n=1}^{N} C^{n}</script>

<blockquote>
  <p><strong>证明</strong> ：$\Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right) \leq C^{n}$</p>

  <p>首先，$C^{n}=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)$
由 $\tilde{y}^{n}=\arg \max _{y} w \cdot \phi\left(x^{n}, y\right)$ 知：$\left[w \cdot \phi\left(x^{n}, \tilde{y}^{n}\right)-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)\right] \geq 0$
所以：
<script type="math/tex">% <![CDATA[
\begin{aligned} 
\Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right) 
&\leq \Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right)+\left[w \cdot \phi\left(x^{n}, \tilde{y}^{n}\right)-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)\right] \\ 
&=\left[\Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right)+w \cdot \varphi\left(x^{n}, \tilde{y}^{n}\right)\right]-w \cdot \varphi\left(x^{n}, \hat{y}^{n}\right)\\ 
& \leq \max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \varphi\left(x^{n}, y\right)\right]-w \cdot \varphi\left(x^{n}, \hat{y}^{n}\right)\\ 
&=C^{n} \end{aligned} %]]></script></p>
</blockquote>

<h3 id="其他代价函数">其他代价函数</h3>

<p>要使得 $\Delta\left(\hat{y}^{n}, \tilde{y}^{n}\right) \leq C^{n}$，也可以选择其他代价函数，比如：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\frac{\text {Margin rescaling:}}{C^{n}=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)}} \\ \\ {\frac{\text {Slack variable rescaling:}}{C^{n}=\max _{\Delta} \Delta\left(\hat{y}^{n}, y\right)\left[1+w \cdot \phi\left(x^{n}, y\right)-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)\right]}}\end{array}</script>

<h2 id="正则化">正则化</h2>

<script type="math/tex; mode=display">C=\sum_{n=1}^{N} C^{n} \quad \longrightarrow \quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} C^{n}</script>

<p>其中，$C^{n}=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \widehat{y}^{n}\right)$。</p>

<p>每一轮迭代，选择训练数据 ${x^{n}, \hat{y}^{n}}$：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\overline{y}^{n}=\operatorname{argmax}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]} \\ \\
{\nabla C^{n}=\phi\left(x^{n}, \overline{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)+w} \\ \\
{w \rightarrow w-\eta\left[\phi\left(x^{n}, \overline{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)\right]-\eta w} \\ \\
{\quad=(1-\eta) w-\eta\left[\phi\left(x^{n}, \overline{y}^{n}\right)-\phi\left(x^{n}, \hat{y}^{n}\right)\right]}\end{array}</script>

<h2 id="结构化-svm--structured-svm-">结构化 SVM （ Structured SVM ）</h2>

<h3 id="目标">目标</h3>

<p>由之前的工作，得到：</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \text { Find } & \text { minimizing } C \\ C &=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} C^{n} \\ C^{n} &=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right) \end{aligned} %]]></script>

<p>即：</p>

<script type="math/tex; mode=display">C^{n}+w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)=\max _{y}\left[\Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)\right]</script>

<p>因为目标是最小化 $C$，故其等价于：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { For } \forall y :} \\ \\
{\quad C^{n}+w \cdot \phi\left(x^{n}, \hat{y}^{n}\right) \geq \Delta\left(\hat{y}^{n}, y\right)+w \cdot \phi\left(x^{n}, y\right)} \\ \\
{\quad w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)-w \cdot \phi\left(x^{n}, y\right) \geq \Delta\left(\hat{y}^{n}, y\right)-C^{n}}\end{array}</script>

<p>也就是，最小化的目标描述等价于：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { Find } w, \varepsilon^{1}, \cdots, \varepsilon^{N} \text { minimizing } C} \\ \\ 
{\quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} \varepsilon^{n}} \\ \\
 {\text { For } \forall n :} \\ \\
{\quad \text { For } \forall y :} \\ \\
 {\quad \quad w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)-w \cdot \phi\left(x^{n}, y\right) \geq \Delta\left(\hat{y}^{n}, y\right)-\varepsilon^{n}}\end{array}</script>

<p>其中的 $\varepsilon^{n}$ 称为松弛变量（ slack variable ）。</p>

<p>因为 $\mathrm{y}=\hat{y}^{n}$ 时，有 $w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)-w \cdot \phi\left(x^{n}, \hat{y}^{n}\right) \geq \Delta\left(\hat{y}^{n}, \hat{y}^{n}\right)-\varepsilon^{n}$，故： $\varepsilon^{n} \geq 0$。所以，原来的目标转化为：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { Find } w, \varepsilon^{1}, \cdots, \varepsilon^{N} \text { minimizing } C} \\ \\ 
{\quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} \varepsilon^{n}} \\ \\
 {\text { For } \forall n :} \\ \\
{\quad \text { For } \forall y \neq \hat{y}^{n} :} \\ \\
 {\quad \quad w \cdot\left(\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, y\right)\right) \geq \Delta\left(\hat{y}^{n}, y\right)-\varepsilon^{n}, \varepsilon^{n} \geq 0}\end{array}</script>

<h3 id="例子">例子</h3>

<p>举个直观的例子来解释下这个目标。考虑下图目标检测的例子：</p>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt2hulekj30hk0ahjrr.jpg" alt="" /></p>

<p>我们需要每个正确的方框和错误的方框之间的距离 满足：</p>

<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4gt2vsjfcj30oz06o3z0.jpg" alt="" /></p>

<p>但是可能并没有一个 $w$ 会满足这样的条件，所以加入松弛因子来放宽这个限制：</p>

<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4gt33pjfbj30pp06egm5.jpg" alt="" /></p>

<p>当然，我们希望 $\varepsilon$ 的值越小越好。</p>

<h3 id="求解">求解</h3>

<p>由上文可以得到目标是：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { Find } w, \varepsilon^{1}, \cdots, \varepsilon^{N} \text { minimizing } C} \\ \\ 
{\quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} \varepsilon^{n}} \\ \\
 {\text { For } \forall n :} \\ \\
{\quad \text { For } \forall y \neq \hat{y}^{n} :} \\ \\
 {\quad \quad w \cdot\left(\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, y\right)\right) \geq \Delta\left(\hat{y}^{n}, y\right)-\varepsilon^{n}, \varepsilon^{n} \geq 0}\end{array}</script>

<p>这是一个 Quadratic Programming (QP) Problem，可以使用各种 QP Problem Solver 来求解，比如 SVM package 的 Solver。但是上述公式中，约束很多，需要寻找一种办法来求解这个问题。</p>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt3deqb2j30qi0ja40h.jpg" alt="" /></p>

<h2 id="结构化-svm-中的切面算法-cutting-plane-algorithm-">结构化 SVM 中的切面算法（ Cutting Plane Algorithm ）</h2>

<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4gt3nesiij30bb09zq33.jpg" alt="" /></p>
<blockquote>
  <p>图：Depiction of the learning QP and approximated feasibility region <a href="http://abnerguzman.com/publications/gkb_aistats13.pdf">来源</a></p>
</blockquote>

<p>如上图，解空间约束在一个 “ 平面 ” 内，这个平面带着很多约束。我们将其抽象为下图 $\left(w, \varepsilon^{1}, \ldots, \varepsilon^{N}\right)$ 组成的参数空间。（简化为二维）</p>

<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4gt3yxzmbj30ci0cggme.jpg" alt="" /></p>

<p>在不带约束的情况下，最优解容易求得在右下角的蓝点处。但是因为约束（图中各种直线），使得难以求得最优解。</p>

<p>假设约束下可选参数范围限制在图中间的钻石形状区域。可以看出，大部分约束是无关紧要的（比如图中的绿色线）。所以，如果我们能将 $y$ 限制带一个小集合 $A^{n}$ （称作 Working Set）中，那么将会大大简化求解过程。</p>

<p>找到 $A^{n}$ 的过程如下：</p>

<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4gt47gu6sj30oe0eet9z.jpg" alt="" /></p>

<p>由上可知，找 $A^{n}$ 的主要思路就是每次找到一些元素加入到 $A^{n}$ 中。首先，初始化 $\mathbb{A}^{1} \ldots \mathbb{A}^{N}$，当 $y$ 限制在一个小集合 $\mathbb{A}^{n} $ 时，就可以当做一个 QP Problem 来求解了。然后，我们求得 $w$ ，将新成员加入 $\mathbb{A}^{n} $，然后循环求解。</p>

<p>以下图解这个过程。</p>

<p>首先，初始化 $A^{n} = null$ （没有约束），然后求解这个 QP Problem，得到  $w$ 在蓝色星星处。</p>

<p>然后，寻找没有满足的约束（图中红色线），从中挑选出 the most violated one （下文讲解如何找出）—— 假设是 $\mathrm{y}^{\prime}$ ，将它加入  $\mathbb{A}^{n} $ ：
<script type="math/tex">\begin{equation}
\mathrm{A}^{n}=\mathrm{A}^{n} \cup\left\{y^{\prime}\right\}
\end{equation}</script>。</p>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt4gko3rj30b50a9wez.jpg" alt="" /></p>

<p>重复这个过程，直到求的最优解 w3 处蓝色星星所在位置。</p>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt4oz0k6j30aj07vt95.jpg" alt="" /></p>

<p>从当前 $A^{n}$ 给定 $\mathrm{w}^{\prime} $ 和 $ \varepsilon^{\prime}$， 如何找到 the most violated one 呢？</p>

<p>由约束条件 $w \cdot(\phi(x, \hat{y})-\phi(x, y)) \geq \Delta(\hat{y}, y)-\varepsilon$ 可知，不满足于约束意味着 $w^{\prime} \cdot(\phi(x, \hat{y})-\phi(x, y))&lt;\Delta(\hat{y}, y)-\varepsilon^{\prime}$，所以衡量 Violation 的程度为：</p>

<script type="math/tex; mode=display">\Delta(\hat{y}, y)-\varepsilon^{\prime}-w^{\prime} \cdot(\phi(x, \hat{y})-\phi(x, y))</script>

<p>因为 $\varepsilon^{\prime}$ 、$w^{\prime}$ 是 fixed 的，$\phi(x, \hat{y})$ 也不影响计算，上式等同于：</p>

<script type="math/tex; mode=display">\Delta(\hat{y}, y)+w^{\prime} \cdot \phi(x, y)</script>

<p>所以，The most violated one 是：</p>

<script type="math/tex; mode=display">\arg \max _{y}[\Delta(\hat{y}, y)+w \cdot \phi(x, y)]</script>

<p>综上，利用切面算法求解的过程如下：</p>

<p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4gt4xqe0mj30na0ghjsh.jpg" alt="" /></p>

<p>其中的 QP 为：</p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt56pvd9j30m807wt93.jpg" alt="" /></p>

<p>以下图解这个过程：</p>

<p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4gt5hin68j30ob0i5gmg.jpg" alt="" /></p>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt5rtj1xj30pi0jftal.jpg" alt="" /></p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt628neqj30od0j675s.jpg" alt="" /></p>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt6aiga5j30pl0jc765.jpg" alt="" /></p>

<h2 id="多分类-svm">多分类 SVM</h2>

<ul>
  <li>Problem 1: Evaluation</li>
</ul>

<p>如果有 K 个分类，那么就有 K 个 weight vectors <script type="math/tex">\left\{w^{1}, w^{2}, \cdots, w^{K}\right\}</script>。那么有：</p>

<script type="math/tex; mode=display">\begin{array}{l}{y \in\{1,2, \cdots, k, \cdots, K\}} \\ \\
{F(x, y)=w^{y} \cdot \vec{x}} \\ \\
{\vec{x} : \text { vector representation of x}}\end{array}</script>

<p>表示成：</p>

<script type="math/tex; mode=display">F(x, y)=w \cdot \phi(x, y)</script>

<p>其中：</p>

<script type="math/tex; mode=display">w=\left( \begin{array}{c}{w^{1}} \\ {w^{2}} \\ {\vdots} \\ {w^{k}} \\ {\vdots} \\ {w^{K}}\end{array}\right) 
\phi(x, y)=\left( \begin{array}{c}{0} \\ {0} \\ {\vdots} \\ {\vec{x}} \\ {\vdots} \\ {0}\end{array}\right)</script>

<ul>
  <li>Problem 2: Inference</li>
</ul>

<script type="math/tex; mode=display">\begin{array}{l}{F(x, y)=w^{y} \cdot \vec{x}} \\ \\ 
{\hat{y}=\arg \max _{y \in\{1,2, \cdots, k, \cdots, K\}} F(x, y)} \\\\ 
{\quad=\arg \max _{y \in\{1,2, \cdots, k, k\}} w^{y} \cdot \vec{x}}\end{array}</script>

<p>一般来说，分类都是比较少的，所以可以直接枚举。</p>

<ul>
  <li>Problem 3: Training</li>
</ul>

<p>由前边可知：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { Find } w, \varepsilon^{1}, \cdots, \varepsilon^{N} \text { minimizing } C} \\ \\ 
{\quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} \varepsilon^{n}} \\ \\
 {\text { For } \forall n :} \\ \\
{\quad \text { For } \forall y \neq \hat{y}^{n} :} \\ \\
 {\quad \quad w \cdot\left(\phi\left(x^{n}, \hat{y}^{n}\right)-\phi\left(x^{n}, y\right)\right) \geq \Delta\left(\hat{y}^{n}, y\right)-\varepsilon^{n}, \varepsilon^{n} \geq 0}\end{array}</script>

<p>因为：</p>

<script type="math/tex; mode=display">\begin{array}{l}{w \cdot \phi\left(x^{n}, \hat{y}^{n}\right)=w^{\hat{y}^{n}} \cdot \vec{x}} \\ {w \cdot \phi\left(x^{n}, y\right)=w^{y} \cdot \vec{x}}\end{array}</script>

<p>故调整为：</p>

<script type="math/tex; mode=display">\begin{array}{l}{\text { Find } w, \varepsilon^{1}, \cdots, \varepsilon^{N} \text { minimizing } C} \\ \\ 
{\quad C=\frac{1}{2}\|w\|^{2}+\lambda \sum_{n=1}^{N} \varepsilon^{n}} \\ \\
 {\text { For } \forall n :} \\ \\
{\quad \text { For } \forall y \neq \hat{y}^{n} :} \\ \\
 {\quad \quad \left(w^{\hat{y}^{n}}-w^{y}\right) \cdot \vec{x} \geq \Delta\left(\hat{y}^{n}, y\right)-\varepsilon^{n}, \varepsilon^{n} \geq 0}\end{array}</script>

<p>因为 $\Delta\left(\hat{y}^{n}, y\right)$ 代表的只是 <code class="highlighter-rouge">N * ( K-1 )</code> 个约束，所以可以自行定义一些约束，比如：</p>

<script type="math/tex; mode=display">\begin{array}{c}{y \in\{d o g, c a t, b u s, c a r\}} \\ {\Delta\left(\hat{y}^{n}=\operatorname{dog}, y=c a t\right)=1} \\ {\Delta\left(\hat{y}^{n}=\operatorname{dog}, y=b u s\right)=100} \\ {\quad \text { (defined as your wish) }}\end{array}</script>

<h2 id="二分类-svm">二分类 SVM</h2>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt6kzolhj30p70e4dgj.jpg" alt="" /></p>

<h2 id="beyond-structured-svm">Beyond Structured SVM</h2>

<ul>
  <li>使用 DNN 生成特征</li>
</ul>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt6ujrrxj30nm06v0st.jpg" alt="" /></p>

<blockquote>
  <p>Ref: Hao Tang, Chao-hong Meng, Lin-shan Lee, “An initial attempt for phoneme
recognition using Structured Support Vector Machine (SVM),” ICASSP, 2010
Shi-Xiong Zhang, Gales, M.J.F., “Structured SVMs for Automatic Speech
Recognition,” in Audio, Speech, and Language Processing, IEEE Transactions on,
vol.21, no.3, pp.544-555, March 2013</p>
</blockquote>

<ul>
  <li>联合训练 SVM 和 DNN</li>
</ul>

<p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4gt727ztuj30o2091aab.jpg" alt="" /></p>

<blockquote>
  <p>Ref: Shi-Xiong Zhang, Chaojun Liu, Kaisheng Yao, and Yifan Gong, “DEEP NEURAL SUPPORT VECTOR MACHINES FOR SPEECH RECOGNITION”, Interspeech 2015</p>
</blockquote>

<ul>
  <li>使用结构化 SVM 代替 DNN</li>
</ul>

<p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4gt7a2gflj30nx06omxb.jpg" alt="" /></p>

<blockquote>
  <p>Ref: Yi-Hsiu Liao, Hung-yi Lee, Lin-shan Lee, “Towards Structured Deep Neural Network for Automatic Speech Recognition”, ASRU, 2015</p>
</blockquote>

<p>有：</p>

<script type="math/tex; mode=display">C=\frac{1}{2}\|\theta\|^{2}+\frac{1}{2}\left\|\theta^{\prime}\right\|^{2}+\lambda \sum_{n=1}^{N} C^{n}</script>

<script type="math/tex; mode=display">C^{n}=\max _{v}\left[\Delta\left(\hat{y}^{n}, y\right)+F\left(x^{n}, y\right)\right]-F\left(x^{n}, \hat{y}^{n}\right)</script>

<h2 id="参考资料">参考资料</h2>

<ol>
  <li><a href="https://www.youtube.com/watch?v=YjvGVVrCrhQ&amp;list=PLJV_el3uVTsNHQKxv49vpq7NSn-zim18V&amp;index=3">台灣大學 李宏毅 《Structured Learning 3: Structured SVM》</a></li>
</ol>


  </section>
</article>

<section class="read-more">
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">最近的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2019/05/struc-04/" title="link to 结构化学习（4）：序列标记（ Sequence Labeling ）">结构化学习（4）：序列标记（ Sequence Labeling ）</a></h2>
       <p class="excerpt">概述序列标记问题，是一个序列到另一个序列的映射，表达为下式：序列标记有很多应用，比如 POS tagging、Morphosyntactic Attributes、Named Entity Recognition、Tokenization、Code switching 和 Dialogue acts 等。以 POS tagging 为例，如下：Hidden Markov Model (HMM)生成句子的步骤我们如何生成一个句子呢？考虑以下步骤：  根据语法生成 POS 序列  根据 POS...&hellip;</p>
       <div class="post-list__meta"><time datetime="2019-05-27 14:50:01 +0800" class="post-list__meta--date date">2019-05-27</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2019/05/struc-04/>继续阅读</a></div>
   </div>
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2019/05/struc-02/" title="link to 结构化学习（2）：结构化线性模型">结构化学习（2）：结构化线性模型</a></h2>
       <p class="excerpt">上一篇文章 《结构化学习（1）：介绍、统一框架》 的最后说到，对于 Unified Framework 我们需要解决三个问题：      Problem 1: 评估 —— 定义 F(x,y)        Problem 2: 推断 —— 计算 “arg max”        Problem 3: 训练 —— 对于给定训练数据，找到 F(x,y)  考虑问题 1 —— 如果 F(x, y) 是一个确定的形式，那么问题 3 将不再是一个问题。以下，我们使用一个结构化的线性模型来考虑这个问题...&hellip;</p>
       <div class="post-list__meta"><time datetime="2019-05-25 19:50:01 +0800" class="post-list__meta--date date">2019-05-25</time> &#8226; <span class="post-list__meta--tags tags"></span><a class="btn-border-small" href=/2019/05/struc-02/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  

  

  <!-- Gitalk 评论 start  -->
  
    <!-- Link Gitalk 的支持文件  -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

    <div id="gitalk-container"></div>

    <script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '6005df5e831456f4482b',
        clientSecret: 'ffc57d242198e14cdb68b7457a765d0b719415fb',
        repo: 'caotouchan.github.io',
        owner: 'CaoTouChan',
        admin: ['CaoTouChan'],
        id: window.location.pathname,
      });
      gitalk.render('gitalk-container');
    </script>
  
  <!-- Gitalk end -->
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">本站由 <a href="https://caotouchan.github.io">@caotouchan</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/CaoTouChan/caotouchan.github.io">本站源码</a> - &copy; 2019</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>
