---
layout: post
title: 结构化学习（4）：序列标记（ Sequence Labeling ）
date: 2019-05-27 15:50:01.000000000 +09:00
---
## 概述

序列标记问题，是一个序列到另一个序列的映射，表达为下式：

![](http://ww3.sinaimg.cn/large/006tNc79gy1g4gtpu974wj30dj07sdft.jpg)

序列标记有很多应用，比如 POS tagging、Morphosyntactic Attributes、Named Entity Recognition、Tokenization、Code switching 和 Dialogue acts 等。

以 POS tagging 为例，如下：

![](http://ww2.sinaimg.cn/large/006tNc79gy1g4gtq3wc0pj30hr07r3yw.jpg)

## Hidden Markov Model (HMM)

### 生成句子的步骤

我们如何生成一个句子呢？考虑以下步骤：

1. 根据语法生成 POS 序列
2. 根据 POS 序列生成句子 （句子的单词从词典抽取）

![](http://ww4.sinaimg.cn/large/006tNc79gy1g4gtqbwu95j309i07gaa1.jpg)

举例来说，假设有以下 POS 转化关系（马尔柯夫链）：

![](http://ww1.sinaimg.cn/large/006tNc79gy1g4gtqilpboj30k70dydgs.jpg)

步骤二转化概率如下：

![](http://ww2.sinaimg.cn/large/006tNc79gy1g4gtqp6kfzj30je0dawf3.jpg)

综上，可描述为：

![](http://ww3.sinaimg.cn/large/006tNc79gy1g4gtqvpdmcj30g00aojrn.jpg)

不失一般性的，我们有：

$$P(x, y)=P(y) P(x | y)
$$

** Step 1 : Transition probability **

$$P(y)=P\left(y_{1} | s t a r t\right) \times \prod_{l=1}^{L-1} P\left(y_{l+1} | y_{l}\right) \times P\left(e n d | y_{L}\right)
$$

** Step 2：Emission probability **

$$P(x | y)=\prod_{l=1}^{L} P\left(x_{l} | y_{l}\right)
$$

考虑上边的例子，可以使用计数来计算 
$$P(x, y)$$
：

![](http://ww1.sinaimg.cn/large/006tNc79gy1g4gtrckhy9j30ip08174n.jpg)

### 用 HMM 处理 POS tagging

POS tagging 问题，如下图，就是给定 
$$x$$
 去求解可能性最大的 
$$y$$
 :

![](http://ww3.sinaimg.cn/large/006tNc79gy1g4gtrnoxp5j308105d748.jpg)

$$\begin{aligned} y &=\arg \max _{y \in Y} P(y | x) \\ &=\arg \max _{y \in Y} \frac{P(x, y)}{P(x)} \\ &=\arg \max _{y \in \mathbb{Y}} P(x, y) \end{aligned}
$$

> 注意：P(x) 固定，对结果无影响；由上一小节可知 P(x,y) 可以计算得到

所以，我们的目标是：

$$\tilde{y}=\arg \max _{y \in \mathbb{Y}} P(x, y)
$$
假设有 
$$|S|$$
 个 tag，序列 
$$y$$
 的长度为 
$$L$$
，则枚举所有 
$$y$$
 共有 
$$|s|^{L}$$
 个。但是，可以使用 **Viterbi algorithm** 求解这个问题，时间复杂度仅为 
$$O\left(L|S|^{2}\right)$$
 。

### 结构化学习中的 HMM

将 HMM 对应到结构化学习的统一框架：

* Problem 1: Evaluation

$$F(x, y)=P(x, y)=P(y) P(x | y)
$$

* Problem 2: Inference

$$\tilde{y}=\arg \max _{y \in \mathbb{Y}} P(x, y)
$$

* Problem 3: Training

$$ P(y) 和 P(x|y) 可以直接从训练数据集中得到$$

### HMM 的问题

在 Inference 阶段：
$$\tilde{y}=\arg \max _{y \in \mathbb{Y}} P(x, y)$$


为了获得最佳答案，需要：
$$(x, \hat{y}) : P(x, \hat{y})>P(x, y)$$


但是这一点，HMM 无法保证。考虑以下例子：

![](http://ww2.sinaimg.cn/large/006tNc79gy1g4gtrwybg0j30j505p74d.jpg)

通过计算，会得到最终路径是：N-> V -> A

这种情形实际上就是说一些在训练数据中从未见过的 
$$(x,y)$$
 可能会获得更大的 
$$P(x,y)$$
。（当然，这一点对于小数据集可能是有利的）

对于这个问题（Transition 和 Emission 的 model 是 independent 的），条件随机场 ( CRF ) 可以使用相同模型进行解决。

## Conditional Random Field (CRF)

### 模型描述

CRF 模型的描述是 （下一小节详述）：

$$\mathrm{P}(x, y) \propto \exp (w \cdot \phi(x, y))
$$

其中，
$$\phi(x, y)$$
 是一个特征向量，
$$w$$
 是需要从训练集中学习的 weight vector， 
$$\exp (w \cdot \phi(x, y)) $$
  是正数（可能会大于 1，这和概率 [0, 1] 的约束有冲突）

我们关心的得是 
$$P(y | x)$$
 ，有：

$$P(y | x)=\frac{P(x, y)}{\sum_{y^{\prime}} P\left(x, y^{\prime}\right)}
$$

因为 
$$P(x, y) $$
 可以写作：

$$\mathrm{P}(x, y)=\frac{\exp (w \cdot \phi(x, y))}{R}
$$

则：

$$P(y | x) = \frac{\exp (w \cdot \phi(x, y))}{\sum_{y^{\prime} \in \mathbb{Y}} \exp \left(w \cdot \phi\left(x, y^{\prime}\right)\right)}=\frac{\exp (w \cdot \phi(x, y))}{Z(x)}
$$

### 和 HMM 的关系

在 HMM 中：

$$P(x, y)=P\left(y_{1} | s t a r t\right) \prod_{l=1}^{L-1} P\left(y_{l+1} | y_{l}\right) P\left(e n d | y_{L}\right) \prod_{l=1}^{L} P\left(x_{l} | y_{l}\right)
$$

即：

$$\begin{array}{l}{\log P(x, y)} \\ \\
{=\log P\left(y_{1} | s t a r t\right)+\sum_{l=1}^{L-1} \log P\left(y_{l+1} | y_{l}\right)+\log P\left(e n d | y_{L}\right)} \\ \\ 
{+\sum_{l=1}^{L} \log P\left(x_{l} | y_{l}\right)}\end{array}
$$

转换一下最后一项：

$$\sum_{l=1}^{L} \log P\left(x_{l} | y_{l}\right)=\sum_{s, t} \log P(t | s) \times N_{s, t}(x, y)
$$

其中，s 为 tag，t 为 word， 
$$N_{s, t}(x, y)$$
 为 s 和 t  在 (x, y) 中一起出现的计数。

举个例子：我们有以下 
$$(x, y)$$
：

![](http://ww2.sinaimg.cn/large/006tNc79gy1g4gts7bz0aj30li0683yk.jpg)

则：

$$\begin{array}{c}{N_{D, t h e}(x, y)=2} \\ {N_{N, d o g}(x, y)=1} \\ {N_{V, a t e}(x, y)=1} \\ {N_{N, h o m e w o r k}(x, y)=1} \\ {N_{s, t}(x, y)=0} \\ \text{(for any other s and t)}\end{array}
$$

则：

$$\begin{array}{l}
{\sum_{l=1}^{L} \log P\left(x_{l} | y_{l}\right)} \\ \\
{\quad=\log P(\operatorname{the} | D)+\log P(d o g | N)+\log P(\text {ate} | V)} \\ \\
{\quad \quad+\log P(\text {the} | D)+\log P(\text {homework} | N)} \\ \\
{\quad=\log P(\text {the} | D) \times 2+\log P(\operatorname{dog} | N) \times 1+\log P(a t e | V) \times 1} \\ \\
{\quad \quad+\log P(\text {homework} | N) \times 1} \\ \\
{\quad=\sum_{s, t} \log P(t | s) \times N_{s, t}(x, y)} \\ \\
\end{array}
$$

$$\log P\left(y_{1} | s t a r t\right)=\sum_{s} \log P(s | \text {start}) \times N_{\text {start}, s}(x, y)
$$

$$\sum_{l=1}^{L-1} \log P\left(y_{l+1} | y_{l}\right)=\sum_{s, s^{\prime}} \log P\left(s^{\prime} | s\right) \times N_{s, s^{\prime}}(x, y)
$$

$$\log P\left(e n d | y_{L}\right)=\sum_{s} \log P(e n d | s) \times N_{s, e n d}(x, y)
$$

则：

$$\begin{array}{l}{\log P(x, y)} \\ \\ 
{=\sum_{s, t} \log P(t | s) \times N_{s, t}(x, y)} \\\\ 
{+\sum_{s} \log P(s | \text {start}) \times N_{\text {start}, s}(x, y)} \\\\ 
{+\sum_{s, s^{\prime}} \log P\left(s^{\prime} | s\right) \times N_{s, S^{\prime}}(x, y)} \\ \\ 
{+\sum_{s} \log P(e n d | s) \times N_{s, e n d}(x, y)} \\\\ 
{= \left[ \begin{array}{c}{\vdots} \\ {\log P(t | s)} \\ {\vdots} \\ {\log P(s | s t a r t)} \\ {\vdots} \\ {\log P\left(s^{\prime} | s\right)} \\ {\vdots}  \\ {\log P(\text {end} | s)} \\ {\vdots}\end{array}\right]   \cdot \left[ \begin{array}{c}{\vdots} \\ {N_{s, t}(x, y)} \\ {\vdots} \\ {N_{start, s}(x, y)} \\ {\vdots} \\ {N_{s, s^{\prime}}(x, y)} \\ {\vdots} \\ {N_{s, end}(x, y)}  \\ {\vdots}\end{array}\right]} \\\\ 
{=w \cdot \phi(x, y)} 
\end{array}
$$

故有：

$$\mathrm{P}(x, y)=\exp (w \cdot \phi(x, y))
$$

### Feature Vector

### Training Criterion

### CRF v.s. HMM

## Structured Perceptron/SVM


## 参考资料

[1].  [台灣大學 李宏毅 《Structured Learning 4: Sequence Labeling》](https://www.youtube.com/watch?v=o9FPSqobMys&list=PLJV_el3uVTsNHQKxv49vpq7NSn-zim18V&index=4)


